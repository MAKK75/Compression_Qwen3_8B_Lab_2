{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 2. Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-22T10:25:58.784856Z",
     "iopub.status.busy": "2025-12-22T10:25:58.784587Z",
     "iopub.status.idle": "2025-12-22T10:26:15.967487Z",
     "shell.execute_reply": "2025-12-22T10:26:15.966651Z",
     "shell.execute_reply.started": "2025-12-22T10:25:58.784831Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting autoawq==0.2.9\n",
      "  Downloading autoawq-0.2.9.tar.gz (74 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting transformers==4.51.3\n",
      "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.4.1)\n",
      "Requirement already satisfied: triton in /usr/local/lib/python3.12/dist-packages (from autoawq==0.2.9) (3.4.0)\n",
      "Requirement already satisfied: tokenizers>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from autoawq==0.2.9) (0.22.1)\n",
      "Requirement already satisfied: typing_extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from autoawq==0.2.9) (4.15.0)\n",
      "Requirement already satisfied: zstandard in /usr/local/lib/python3.12/dist-packages (from autoawq==0.2.9) (0.25.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.26.5 in /usr/local/lib/python3.12/dist-packages (from autoawq==0.2.9) (0.36.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.51.3) (3.20.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.51.3) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.51.3) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.51.3) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.51.3) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.51.3) (2.32.5)\n",
      "Collecting tokenizers>=0.12.1 (from autoawq==0.2.9)\n",
      "  Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers==4.51.3) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.51.3) (4.67.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.12.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.26.5->autoawq==0.2.9) (1.2.1rc0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.51.3) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.51.3) (2.6.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m89.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m103.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: autoawq\n",
      "  Building wheel for autoawq (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for autoawq: filename=autoawq-0.2.9-py3-none-any.whl size=115106 sha256=dd2df7a452ee1200e358c486b889715cb2fa0f267915a7aa1ce04dcf30102644\n",
      "  Stored in directory: /root/.cache/pip/wheels/45/1a/7b/7314b3a958454e8ce349f600829a3f0a6a05aeebf987be1e16\n",
      "Successfully built autoawq\n",
      "Installing collected packages: tokenizers, transformers, autoawq\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.22.1\n",
      "    Uninstalling tokenizers-0.22.1:\n",
      "      Successfully uninstalled tokenizers-0.22.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.57.1\n",
      "    Uninstalling transformers-4.57.1:\n",
      "      Successfully uninstalled transformers-4.57.1\n",
      "Successfully installed autoawq-0.2.9 tokenizers-0.21.4 transformers-4.51.3\n"
     ]
    }
   ],
   "source": [
    "!pip install autoawq==0.2.9 transformers==4.51.3 torch accelerate datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Квантование (int4), калибровка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-22T10:40:35.607466Z",
     "iopub.status.busy": "2025-12-22T10:40:35.606489Z",
     "iopub.status.idle": "2025-12-22T11:23:44.273265Z",
     "shell.execute_reply": "2025-12-22T11:23:44.272362Z",
     "shell.execute_reply.started": "2025-12-22T10:40:35.607426Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/awq/__init__.py:21: DeprecationWarning: \n",
      "I have left this message as the final dev message to help you transition.\n",
      "\n",
      "Important Notice:\n",
      "- AutoAWQ is officially deprecated and will no longer be maintained.\n",
      "- The last tested configuration used Torch 2.6.0 and Transformers 4.51.3.\n",
      "- If future versions of Transformers break AutoAWQ compatibility, please report the issue to the Transformers project.\n",
      "\n",
      "Alternative:\n",
      "- AutoAWQ has been adopted by the vLLM Project: https://github.com/vllm-project/llm-compressor\n",
      "\n",
      "For further inquiries, feel free to reach out:\n",
      "- X: https://x.com/casper_hansen_\n",
      "- LinkedIn: https://www.linkedin.com/in/casper-hansen-804005170/\n",
      "\n",
      "  warnings.warn(_FINAL_DEV_MESSAGE, category=DeprecationWarning, stacklevel=1)\n",
      "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.utcnow().replace(tzinfo=utc)\n",
      "2025-12-22 10:40:47.192801: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1766400047.370887      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1766400047.420012      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1766400047.827892      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1766400047.827938      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1766400047.827941      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1766400047.827943      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.utcnow().replace(tzinfo=utc)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Оригинальный размер модели: 15.26 GB\n",
      "\n",
      "Загрузка модели для квантования...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c00777864b1e477c96f7afc81fb1c06f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/728 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c9fac764e464b77abb6a01b6f3b71de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 15 files:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20d9ee6590ba462d9f91f5787cdd630b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea206a03b91a4406ac21127ae79b0812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca77caffcf474b7888ee5c295047483d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04671f688d7948fea4e102e5d95a8687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00005.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8be23c909b784b6f81844207536287cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LICENSE: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07d928867eab40e7aeeb91e46b510d82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40735b7814d44badb95201875d565d68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00005.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b184a0337ee4f0284b92a361ea45ceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00005.safetensors:   0%|          | 0.00/3.19G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "667380fd31094a7aa688298542d24dc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e68b77745cbd47b6b0da6d4002cfd343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00005.safetensors:   0%|          | 0.00/1.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a36844820fe94452abcd3c601294d7ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00005.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa0425e6c2954256bd86c98039df6c3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "751b61ae2f2d4cfd8641aa0c64de20b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f554aa1f90ca4befb726a3010497eedf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f5d7ea3025e4cea83eb8c24bf6b0538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Начало калибровки\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.utcnow().replace(tzinfo=utc)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34af77cc11d642418f5f57c412983d3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/167 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "647a4bce4eac48ea94d2efba36b894d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val.jsonl.zst:   0%|          | 0.00/471M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "238401727873452e98eb9252c188b3a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/214670 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AWQ: 100%|██████████| 36/36 [39:31<00:00, 65.89s/it]\n",
      "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.utcnow().replace(tzinfo=utc)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Сохранение модели в ./qwen3-8b-awq\n",
      "\n",
      "==============================\n",
      "РЕЗУЛЬТАТЫ СЖАТИЯ:\n",
      "Оригинальный размер: 15.26 GB\n",
      "Сжатый размер: 5.68 GB\n",
      "Compression Ratio: 2.69x\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import random\n",
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "from huggingface_hub import model_info\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def get_model_size_gb(model_id):\n",
    "    try:\n",
    "        info = model_info(model_id, files_metadata=True)\n",
    "        siblings = info.siblings\n",
    "        \n",
    "        def get_file_size(file_obj):\n",
    "            if hasattr(file_obj, 'lfs') and file_obj.lfs is not None:\n",
    "                return file_obj.lfs.size\n",
    "            if hasattr(file_obj, 'size') and file_obj.size is not None:\n",
    "                return file_obj.size\n",
    "            return 0\n",
    "\n",
    "        safetensors_size = sum(get_file_size(s) for s in siblings if s.rfilename.endswith('.safetensors'))\n",
    "        bin_size = sum(get_file_size(s) for s in siblings if s.rfilename.endswith('.bin'))\n",
    "        \n",
    "        final_size = safetensors_size if safetensors_size > 0 else bin_size\n",
    "        if final_size == 0: \n",
    "            final_size = sum(get_file_size(s) for s in siblings if any(ext in s.rfilename for ext in ['.pt', '.pth', '.ckpt']))\n",
    "            \n",
    "        return final_size / (1024**3)\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при получении размера: {e}\")\n",
    "        return 0\n",
    "\n",
    "def compress_model():\n",
    "    seed_everything(42)\n",
    "    model_id = \"Qwen/Qwen3-8B\" \n",
    "    quant_path = \"./qwen3-8b-awq\"\n",
    "    \n",
    "    size_orig = get_model_size_gb(model_id)\n",
    "    print(f\"Оригинальный размер модели: {size_orig:.2f} GB\")\n",
    "\n",
    "    quant_config = { \n",
    "        \"zero_point\": True, \n",
    "        \"q_group_size\": 128, \n",
    "        \"w_bit\": 4, \n",
    "        \"version\": \"GEMM\" \n",
    "    }\n",
    "\n",
    "    print(\"\\nЗагрузка модели для квантования...\")\n",
    "    model = AutoAWQForCausalLM.from_pretrained(\n",
    "        model_id, \n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "    print(\"\\nНачало калибровки\")\n",
    "    model.quantize(\n",
    "        tokenizer, \n",
    "        quant_config=quant_config,\n",
    "        n_parallel_calib_samples=1, \n",
    "        max_calib_seq_len=512,       \n",
    "        max_calib_samples=32\n",
    "    )\n",
    "\n",
    "    print(f\"\\nСохранение модели в {quant_path}\")\n",
    "    model.save_quantized(quant_path)\n",
    "    tokenizer.save_pretrained(quant_path)\n",
    "\n",
    "    size_comp = sum(os.path.getsize(os.path.join(quant_path, f)) for f in os.listdir(quant_path) \n",
    "                    if any(ext in f for ext in ['.safetensors', '.bin', '.pt'])) / (1024**3)\n",
    "    \n",
    "    ratio = size_orig / size_comp if size_comp > 0 else 0\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(f\"РЕЗУЛЬТАТЫ СЖАТИЯ:\")\n",
    "    print(f\"Оригинальный размер: {size_orig:.2f} GB\")\n",
    "    print(f\"Сжатый размер: {size_comp:.2f} GB\")\n",
    "    print(f\"Compression Ratio: {ratio:.2f}x\")\n",
    "    print(\"=\"*30)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    compress_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall -y huggingface_hub\n",
    "# !pip install \"huggingface_hub<1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from huggingface_hub import HfApi\n",
    "\n",
    "# HF_TOKEN = \"...\" \n",
    "# USERNAME = \"NOVORDSEC\" \n",
    "# REPO_NAME = \"qwen3-8b-awq-int4\"\n",
    "# LOCAL_DIR = \"./qwen3-8b-awq\"\n",
    "\n",
    "# repo_id = f\"{USERNAME}/{REPO_NAME}\"\n",
    "\n",
    "# api = HfApi(token=HF_TOKEN)\n",
    "\n",
    "# print(f\"Проверка репозитория {repo_id}...\")\n",
    "# try:\n",
    "#     api.create_repo(repo_id=repo_id, repo_type=\"model\", exist_ok=True)\n",
    "#     print(\"Репозиторий готов.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Ошибка при создании: {e}\")\n",
    "\n",
    "# print(\"Загрузка...\")\n",
    "# try:\n",
    "#     api.upload_folder(\n",
    "#         folder_path=LOCAL_DIR,\n",
    "#         repo_id=repo_id,\n",
    "#         repo_type=\"model\"\n",
    "#     )\n",
    "#     print(f\"\\nСсылка: https://huggingface.co/{repo_id}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Ошибка при загрузке: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оценка на 20% MMLU базовой и квантованной моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На 100% MMLU тоже делали, убедились, что результат очень близкий."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-22T14:36:16.453551Z",
     "iopub.status.busy": "2025-12-22T14:36:16.452821Z",
     "iopub.status.idle": "2025-12-22T15:47:22.876112Z",
     "shell.execute_reply": "2025-12-22T15:47:22.875360Z",
     "shell.execute_reply.started": "2025-12-22T14:36:16.453520Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 15.26 GB | Compressed: 5.68 GB | Ratio: 2.69x\n",
      "\n",
      ">>> Оценка ОРИГИНАЛЬНОЙ модели...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4917c0a6ab8a468db6524a32ed3c87f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Original Model:   0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.utcnow().replace(tzinfo=utc)\n",
      "Original Model: 100%|██████████| 57/57 [36:58<00:00, 38.93s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Оценка СЖАТОЙ модели (AWQ)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94e9289ae0504a05a6c63e2914e0f381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Replacing layers...: 100%|██████████| 36/36 [00:15<00:00,  2.26it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/902 [00:00<?, ?w/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?w/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/awq/models/base.py:541: UserWarning: Skipping fusing modules because AWQ extension is not installed.No module named 'awq_ext'\n",
      "  warnings.warn(\"Skipping fusing modules because AWQ extension is not installed.\" + msg)\n",
      "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.utcnow().replace(tzinfo=utc)\n",
      "Compressed Model: 100%|██████████| 57/57 [32:25<00:00, 34.14s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "ИТОГИ ЭТАПА 1:\n",
      "Compression Ratio: 2.6861\n",
      "Baseline Accuracy: 0.7292\n",
      "Compressed Accuracy: 0.7167\n",
      "Performance Drop: 1.72%\n",
      "ФИНАЛЬНЫЙ SCORE: 2.6407\n",
      "========================================\n",
      "Детальный отчет сохранен в mmlu_comparison_detailed.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\n",
    "from awq import AutoAWQForCausalLM\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import model_info\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    set_seed(seed)\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "ORIG_MODEL_ID = \"Qwen/Qwen3-8B\"\n",
    "COMP_MODEL_ID = \"NOVORDSEC/qwen3-8b-awq-int4\" \n",
    "FRACTION = 0.2  \n",
    "\n",
    "def get_model_size_gb(model_id):\n",
    "    try:\n",
    "        info = model_info(model_id, files_metadata=True)\n",
    "        size = sum(s.size for s in info.siblings if s.size and any(ext in s.rfilename for ext in ['.safetensors', '.bin']))\n",
    "        return size / (1024**3)\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка получения размера для {model_id}: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "def run_mmlu_benchmark(model, tokenizer, fraction=0.2, desc=\"Benchmarking\"):\n",
    "    subjects = [\n",
    "        'abstract_algebra', 'anatomy', 'astronomy', 'business_ethics', 'clinical_knowledge', \n",
    "        'college_biology', 'college_chemistry', 'college_computer_science', 'college_mathematics', \n",
    "        'college_medicine', 'college_physics', 'computer_security', 'conceptual_physics', \n",
    "        'econometrics', 'electrical_engineering', 'elementary_mathematics', 'formal_logic', \n",
    "        'global_facts', 'high_school_biology', 'high_school_chemistry', 'high_school_computer_science', \n",
    "        'high_school_european_history', 'high_school_geography', 'high_school_government_and_politics', \n",
    "        'high_school_macroeconomics', 'high_school_mathematics', 'high_school_microeconomics', \n",
    "        'high_school_physics', 'high_school_psychology', 'high_school_statistics', \n",
    "        'high_school_us_history', 'high_school_world_history', 'human_aging', 'human_sexuality', \n",
    "        'international_law', 'jurisprudence', 'logical_fallacies', 'machine_learning', \n",
    "        'management', 'marketing', 'medical_genetics', 'miscellaneous', 'moral_disputes', \n",
    "        'moral_scenarios', 'nutrition', 'philosophy', 'prehistory', 'professional_accounting', \n",
    "        'professional_law', 'professional_medicine', 'professional_psychology', \n",
    "        'public_relations', 'security_studies', 'sociology', 'us_foreign_policy', 'virology', 'world_religions'\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        device = model.device\n",
    "    except AttributeError:\n",
    "        device = next(model.parameters()).device\n",
    "\n",
    "    choices = [\"A\", \"B\", \"C\", \"D\"]\n",
    "    choice_tokens = [tokenizer.encode(f\" {c}\", add_special_tokens=False)[-1] for c in choices]\n",
    "    \n",
    "    detailed_results = {}\n",
    "    total_correct = 0\n",
    "    total_questions = 0\n",
    "    \n",
    "    model.eval()\n",
    "    for subject in tqdm(subjects, desc=desc):\n",
    "        try:\n",
    "            dataset = load_dataset(\"cais/mmlu\", subject, split=\"test\")\n",
    "            num_samples = max(1, int(len(dataset) * fraction))\n",
    "            dataset = dataset.select(range(num_samples))\n",
    "            \n",
    "            sub_correct = 0\n",
    "            for item in dataset:\n",
    "                prompt = f\"The following are multiple choice questions (with answers) about {subject.replace('_', ' ')}.\\n\\n\"\n",
    "                prompt += f\"{item['question']}\\n\"\n",
    "                prompt += f\"(A) {item['choices'][0]}\\n(B) {item['choices'][1]}\\n(C) {item['choices'][2]}\\n(D) {item['choices'][3]}\\n\"\n",
    "                prompt += \"Answer:\"\n",
    "                \n",
    "                inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "                \n",
    "                with torch.inference_mode():\n",
    "                    logits = model(**inputs).logits[0, -1, :]\n",
    "                    relevant_logits = logits[choice_tokens]\n",
    "                    pred = torch.argmax(relevant_logits).item()\n",
    "                    \n",
    "                    if pred == item['answer']:\n",
    "                        sub_correct += 1\n",
    "            \n",
    "            acc = sub_correct / num_samples\n",
    "            detailed_results[subject] = acc\n",
    "            total_correct += sub_correct\n",
    "            total_questions += num_samples\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка в теме {subject}: {e}\")\n",
    "            \n",
    "    return (total_correct / total_questions if total_questions > 0 else 0), detailed_results\n",
    "\n",
    "\n",
    "size_orig = get_model_size_gb(ORIG_MODEL_ID)\n",
    "size_comp = get_model_size_gb(COMP_MODEL_ID)\n",
    "ratio = size_orig / size_comp if size_comp > 0 else 0\n",
    "print(f\"Original: {size_orig:.2f} GB | Compressed: {size_comp:.2f} GB | Ratio: {ratio:.2f}x\")\n",
    "\n",
    "print(\"\\n>>> Оценка ОРИГИНАЛЬНОЙ модели...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(ORIG_MODEL_ID, trust_remote_code=True)\n",
    "model_orig = AutoModelForCausalLM.from_pretrained(\n",
    "    ORIG_MODEL_ID, \n",
    "    torch_dtype=torch.float16, \n",
    "    device_map=\"auto\", \n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "orig_acc, orig_detailed = run_mmlu_benchmark(model_orig, tokenizer, fraction=FRACTION, desc=\"Original Model\")\n",
    "\n",
    "del model_orig\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n>>> Оценка СЖАТОЙ модели (AWQ)...\")\n",
    "model_quant = AutoAWQForCausalLM.from_quantized(\n",
    "    COMP_MODEL_ID, \n",
    "    fuse_layers=True, \n",
    "    device_map=\"auto\", \n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer_quant = AutoTokenizer.from_pretrained(COMP_MODEL_ID, trust_remote_code=True)\n",
    "\n",
    "comp_acc, comp_detailed = run_mmlu_benchmark(model_quant, tokenizer_quant, fraction=FRACTION, desc=\"Compressed Model\")\n",
    "\n",
    "drop = (orig_acc - comp_acc) / orig_acc if orig_acc > 0 else 0\n",
    "score = ratio / (1 + max(0, drop))\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(f\"ИТОГИ ЭТАПА 1:\")\n",
    "print(f\"Compression Ratio: {ratio:.4f}\")\n",
    "print(f\"Baseline Accuracy: {orig_acc:.4f}\")\n",
    "print(f\"Compressed Accuracy: {comp_acc:.4f}\")\n",
    "print(f\"Performance Drop: {drop*100:.2f}%\")\n",
    "print(f\"ФИНАЛЬНЫЙ SCORE: {score:.4f}\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "subjects = sorted(list(orig_detailed.keys()))\n",
    "comparison_data = []\n",
    "for s in subjects:\n",
    "    comparison_data.append({\n",
    "        \"Subject\": s,\n",
    "        \"Original_Acc\": orig_detailed.get(s, 0.0),\n",
    "        \"Compressed_Acc\": comp_detailed.get(s, 0.0),\n",
    "        \"Diff\": orig_detailed.get(s, 0.0) - comp_detailed.get(s, 0.0)\n",
    "    })\n",
    "\n",
    "df_final = pd.DataFrame(comparison_data)\n",
    "df_final.to_csv(\"mmlu_comparison_detailed.csv\", index=False)\n",
    "print(f\"Детальный отчет сохранен в mmlu_comparison_detailed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T14:25:06.829792Z",
     "iopub.status.busy": "2025-12-23T14:25:06.829253Z",
     "iopub.status.idle": "2025-12-23T14:25:06.996969Z",
     "shell.execute_reply": "2025-12-23T14:25:06.996123Z",
     "shell.execute_reply.started": "2025-12-23T14:25:06.829762Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Запрос метаданных для Qwen/Qwen3-8B...\n",
      "Запрос метаданных для NOVORDSEC/qwen3-8b-awq-int4...\n",
      "\n",
      "==================================================\n",
      "Количество параметров:\n",
      "Оригинальная модель: 8.19B (миллиардов)\n",
      "Сжатая модель:     8.19B (миллиардов)\n",
      "Вроде логично, что не упало, мы только квантовали.\n",
      "Изначально забыли посчитать, потому что в формуле участвует размер в гигабайтах\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import model_info\n",
    "\n",
    "def get_params_from_hub(model_id):\n",
    "    print(f\"Запрос метаданных для {model_id}...\")\n",
    "    try:\n",
    "        info = model_info(model_id)\n",
    "        \n",
    "        if hasattr(info, 'safetensors') and info.safetensors is not None:\n",
    "            if 'total' in info.safetensors:\n",
    "                return info.safetensors['total']\n",
    "        \n",
    "        for tag in info.tags:\n",
    "            if tag.startswith(\"region:\"): continue \n",
    "            if \"params:\" in tag:\n",
    "                val = tag.replace(\"params:\", \"\")\n",
    "                if 'B' in val.upper(): return float(val.upper().replace('B', '')) * 1e9\n",
    "                if 'M' in val.upper(): return float(val.upper().replace('M', '')) * 1e6\n",
    "        \n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка доступа к Hub для {model_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "def format_big_num(n):\n",
    "    if n is None: return \"Не удалось определить\"\n",
    "    if n >= 1e9: return f\"{n / 1e9:.2f}B (миллиардов)\"\n",
    "    if n >= 1e6: return f\"{n / 1e6:.2f}M (миллионов)\"\n",
    "    return f\"{n:,}\"\n",
    "\n",
    "ORIG_ID = \"Qwen/Qwen3-8B\"\n",
    "COMP_ID = \"NOVORDSEC/qwen3-8b-awq-int4\"\n",
    "\n",
    "p_orig = get_params_from_hub(ORIG_ID)\n",
    "p_comp = get_params_from_hub(COMP_ID)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"Количество параметров:\")\n",
    "print(f\"Оригинальная модель: {format_big_num(p_orig)}\")\n",
    "print(f\"Сжатая модель:     {format_big_num(p_comp)}\")\n",
    "print(f\"Вроде логично, что не упало, мы только квантовали.\")\n",
    "print(f\"Изначально забыли посчитать, потому что в формуле участвует размер в гигабайтах\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
